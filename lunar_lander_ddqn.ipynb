{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landing on the Moon with Deep Reinforcement Learning\n",
    "## Solving Lunar Lander using Double Deep Q-Networks (DDQN)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook presents a complete implementation of Double Deep Q-Networks (DDQN) for solving the Lunar Lander environment. The task involves landing a spacecraft on the moon's surface while managing limited fuel, controlling multiple thrusters, and making real-time decisions based on continuous state observations.\n",
    "\n",
    "Consider the analogy of learning to land an aircraft. A pilot trainee learns through repeated attempts, receiving feedback from instructors (rewards and penalties), and gradually develops the skills to execute smooth landings. Similarly, our reinforcement learning agent learns through trial and error, experiencing rewards for successful actions and penalties for poor decisions.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understanding the Lunar Lander environment specifications\n",
    "- Foundations of Reinforcement Learning and Q-Learning\n",
    "- The Double DQN algorithm and its advantages over vanilla DQN\n",
    "- Neural network function approximation for value functions\n",
    "- Exploration-exploitation trade-offs in sequential decision making\n",
    "- Experience replay mechanisms for stable learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Dependencies\n",
    "\n",
    "This section initializes the necessary libraries and sets up the computational environment for training the DDQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Core libraries for numerical computation and deep learning\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Core libraries for numerical computation and deep learning\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Gymnasium (formerly OpenAI Gym) - simulation environment\n",
    "import gymnasium as gym\n",
    "\n",
    "# For storing and sampling experiences\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from IPython import display\n",
    "\n",
    "# Utility imports\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check if GPU is available for accelerated training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding the Lunar Lander Environment\n",
    "\n",
    "### Environment Definition\n",
    "\n",
    "The Lunar Lander environment simulates a spacecraft landing task where an agent must control a lander to safely touch down on a designated landing pad. The environment provides continuous state observations and accepts discrete control actions.\n",
    "\n",
    "### State Space\n",
    "\n",
    "**Definition:** The state space is an 8-dimensional continuous vector representing the lander's current configuration.\n",
    "\n",
    "The state vector consists of:\n",
    "1. **x-position**: Horizontal coordinate relative to landing pad\n",
    "2. **y-position**: Vertical coordinate (altitude)\n",
    "3. **x-velocity**: Horizontal velocity component\n",
    "4. **y-velocity**: Vertical velocity component (descent rate)\n",
    "5. **angle**: Angular orientation of the lander\n",
    "6. **angular velocity**: Rate of rotation\n",
    "7. **left leg contact**: Boolean indicator (1 if touching ground, 0 otherwise)\n",
    "8. **right leg contact**: Boolean indicator (1 if touching ground, 0 otherwise)\n",
    "\n",
    "**Example:** A state vector [0.2, 1.5, -0.3, -0.8, 0.1, 0.05, 0, 0] indicates the lander is slightly right of center, at moderate altitude, moving left and downward, with slight tilt, and neither leg is touching the ground.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "**Definition:** The action space is discrete with 4 possible actions corresponding to thruster controls.\n",
    "\n",
    "Available actions:\n",
    "- **Action 0**: Do nothing (coast)\n",
    "- **Action 1**: Fire left orientation engine (rightward thrust)\n",
    "- **Action 2**: Fire main engine (upward thrust)\n",
    "- **Action 3**: Fire right orientation engine (leftward thrust)\n",
    "\n",
    "### Reward Structure\n",
    "\n",
    "**Definition:** The reward function provides scalar feedback for each action, guiding the agent toward successful landing behavior.\n",
    "\n",
    "**Positive Rewards:**\n",
    "- Moving closer to landing pad center: continuous positive reward\n",
    "- Successful landing: +100 to +140 points (based on precision)\n",
    "- Each leg ground contact: +10 points per leg\n",
    "\n",
    "**Negative Rewards (Penalties):**\n",
    "- Moving away from landing pad: continuous negative reward\n",
    "- Main engine firing: -0.3 per timestep\n",
    "- Side engine firing: -0.03 per timestep\n",
    "- Crash landing: -100 points\n",
    "\n",
    "**Episode Termination Conditions:**\n",
    "1. Successful landing (both legs down, velocity below threshold)\n",
    "2. Crash (excessive velocity or extreme angle)\n",
    "3. Out of bounds\n",
    "4. Timeout (1000 timesteps)\n",
    "\n",
    "**Solved Criterion:** The environment is considered solved when the agent achieves an average score of 200 or greater over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Lunar Lander environment\n",
    "# render_mode=\"rgb_array\" enables frame capture for visualization\n",
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "\n",
    "# Extract environment specifications\n",
    "state_size = env.observation_space.shape[0]  # 8 continuous values\n",
    "action_size = env.action_space.n  # 4 discrete actions\n",
    "\n",
    "print(f\"Environment created: LunarLander-v2\")\n",
    "print(f\"State space dimension: {state_size}\")\n",
    "print(f\"Action space dimension: {action_size}\")\n",
    "print(f\"Solved threshold: 200 average reward over 100 episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Reinforcement Learning Foundations\n",
    "\n",
    "### Reinforcement Learning Framework\n",
    "\n",
    "**Definition:** Reinforcement Learning (RL) is a machine learning paradigm where an agent learns to make sequential decisions by interacting with an environment to maximize cumulative reward.\n",
    "\n",
    "**Formal Components:**\n",
    "- **Agent**: The decision-making entity (our neural network)\n",
    "- **Environment**: The external system with which the agent interacts (Lunar Lander simulation)\n",
    "- **State (s)**: Observable configuration of the environment at time t\n",
    "- **Action (a)**: Choice made by the agent from a finite or infinite set\n",
    "- **Reward (r)**: Scalar feedback signal from the environment\n",
    "- **Policy (π)**: Mapping from states to actions, π: S → A\n",
    "\n",
    "**Example:** Consider teaching a dog to sit. The dog (agent) observes its surroundings (state), chooses to sit or not (action), and receives a treat or no treat (reward). Through repeated trials, the dog learns a policy that maximizes treats.\n",
    "\n",
    "### Q-Learning and Value Functions\n",
    "\n",
    "**Definition:** The Q-value function Q(s, a) represents the expected cumulative reward when taking action a in state s and following the optimal policy thereafter.\n",
    "\n",
    "**Bellman Optimality Equation:**\n",
    "\n",
    "$$Q^*(s, a) = \\mathbb{E}\\left[r + \\gamma \\max_{a'} Q^*(s', a') \\mid s, a\\right]$$\n",
    "\n",
    "Where:\n",
    "- $Q^*(s, a)$: Optimal Q-value for state-action pair (s, a)\n",
    "- $r$: Immediate reward\n",
    "- $\\gamma \\in [0, 1]$: Discount factor (typically 0.99)\n",
    "- $s'$: Next state after taking action a\n",
    "- $\\max_{a'} Q^*(s', a')$: Maximum Q-value achievable from next state\n",
    "\n",
    "**Interpretation:** The Q-value decomposes into immediate reward plus the discounted value of the best possible future action. The discount factor γ encodes the preference for immediate rewards over delayed rewards.\n",
    "\n",
    "**Example:** If you are 10 steps from the landing pad and consider firing the main thruster, Q(s, fire_main) estimates the total reward from this action through episode completion, accounting for fuel costs and positioning benefits.\n",
    "\n",
    "### Deep Q-Networks (DQN)\n",
    "\n",
    "**Problem:** Traditional Q-learning maintains a table Q(s, a) for all state-action pairs. For continuous or high-dimensional state spaces, this becomes computationally intractable.\n",
    "\n",
    "**Solution:** Deep Q-Networks approximate the Q-function using a neural network with parameters θ:\n",
    "\n",
    "$$Q(s, a; \\theta) \\approx Q^*(s, a)$$\n",
    "\n",
    "The network takes state s as input and outputs Q-values for all actions: [Q(s, a₀), Q(s, a₁), ..., Q(s, aₙ)].\n",
    "\n",
    "### Overestimation Bias in DQN\n",
    "\n",
    "**Problem:** The standard DQN update uses:\n",
    "\n",
    "$$y = r + \\gamma \\max_{a'} Q(s', a'; \\theta)$$\n",
    "\n",
    "The max operator consistently selects overestimated values due to approximation errors, leading to overoptimistic Q-value estimates that propagate through training.\n",
    "\n",
    "**Example:** If the network incorrectly estimates Q(s', a₁) = 50 when the true value is 30, taking the maximum reinforces this overestimation in subsequent updates.\n",
    "\n",
    "### Double Deep Q-Network (DDQN)\n",
    "\n",
    "**Solution:** DDQN addresses overestimation bias by decoupling action selection from action evaluation using two networks:\n",
    "\n",
    "1. **Online network (θ)**: Actively trained, used for action selection\n",
    "2. **Target network (θ⁻)**: Periodically updated copy, used for action evaluation\n",
    "\n",
    "**DDQN Update Rule:**\n",
    "\n",
    "$$y = r + \\gamma Q(s', \\arg\\max_{a'} Q(s', a'; \\theta); \\theta^-)$$\n",
    "\n",
    "**Process:**\n",
    "1. Online network selects best action: $a^* = \\arg\\max_{a'} Q(s', a'; \\theta)$\n",
    "2. Target network evaluates that action: $Q(s', a^*; \\theta^-)$\n",
    "\n",
    "**Advantage:** By using different networks for selection and evaluation, the bias toward overestimated values is significantly reduced.\n",
    "\n",
    "**Example:** If the online network incorrectly prefers action a₁, the target network provides an independent evaluation that may reveal this action's true (lower) value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Neural Network Architecture\n",
    "\n",
    "### Network Design\n",
    "\n",
    "**Definition:** The Q-network is a fully connected feedforward neural network (multilayer perceptron) that maps state vectors to action-value estimates.\n",
    "\n",
    "**Architecture Specification:**\n",
    "- **Input layer**: 8 neurons (state dimension)\n",
    "- **Hidden layer 1**: 128 neurons with ReLU activation\n",
    "- **Hidden layer 2**: 128 neurons with ReLU activation\n",
    "- **Output layer**: 4 neurons (Q-value for each action)\n",
    "\n",
    "**Activation Function:** ReLU (Rectified Linear Unit) is defined as:\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "ReLU introduces non-linearity while maintaining computational efficiency and mitigating vanishing gradient problems.\n",
    "\n",
    "**Design Rationale:**\n",
    "- 128 hidden units provide sufficient capacity to approximate complex value functions without excessive parameters\n",
    "- Two hidden layers enable hierarchical feature learning\n",
    "- No activation on output layer allows Q-values to span the full real number line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network: Neural network approximation of the Q-function.\n",
    "    \n",
    "    Architecture: 8 -> 128 -> 128 -> 4\n",
    "    Maps state vectors to Q-values for all actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        \"\"\"\n",
    "        Initialize network layers.\n",
    "        \n",
    "        Args:\n",
    "            state_size (int): Input dimension (8 for Lunar Lander)\n",
    "            action_size (int): Output dimension (4 for Lunar Lander)\n",
    "            hidden_size (int): Number of neurons in hidden layers\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Layer definitions\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network.\n",
    "        \n",
    "        Args:\n",
    "            state (torch.Tensor): State vector [batch_size, state_size]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Q-values [batch_size, action_size]\n",
    "        \"\"\"\n",
    "        # First hidden layer with ReLU activation\n",
    "        x = F.relu(self.fc1(state))\n",
    "        \n",
    "        # Second hidden layer with ReLU activation\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Output layer (no activation)\n",
    "        # Q-values can be any real number\n",
    "        q_values = self.fc3(x)\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "# Instantiate both networks\n",
    "policy_net = DQN(state_size, action_size).to(device)  # Online network\n",
    "target_net = DQN(state_size, action_size).to(device)  # Target network\n",
    "\n",
    "# Initialize target network with same weights as policy network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"Neural networks initialized\")\n",
    "print(f\"Architecture: {state_size} -> 128 -> 128 -> {action_size}\")\n",
    "print(f\"Total trainable parameters: {sum(p.numel() for p in policy_net.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Experience Replay Buffer\n",
    "\n",
    "### Motivation\n",
    "\n",
    "**Problem:** Sequential state transitions in RL are temporally correlated. Training on consecutive experiences leads to two issues:\n",
    "\n",
    "1. **Temporal correlation**: Consecutive states are highly similar, violating the i.i.d. (independent and identically distributed) assumption of stochastic gradient descent\n",
    "2. **Catastrophic forgetting**: Neural networks overfit to recent experiences and forget earlier learned patterns\n",
    "\n",
    "**Example:** If you only study the last chapter of a textbook repeatedly, you will master that chapter but forget everything learned earlier. Reviewing random chapters maintains comprehensive knowledge.\n",
    "\n",
    "### Experience Replay Mechanism\n",
    "\n",
    "**Definition:** Experience replay is a technique where experiences (state, action, reward, next_state, done) are stored in a buffer and randomly sampled during training.\n",
    "\n",
    "**Experience Tuple:**\n",
    "\n",
    "$$(s_t, a_t, r_t, s_{t+1}, d_t)$$\n",
    "\n",
    "Where:\n",
    "- $s_t$: State at time t\n",
    "- $a_t$: Action taken\n",
    "- $r_t$: Reward received\n",
    "- $s_{t+1}$: Resulting next state\n",
    "- $d_t$: Terminal flag (1 if episode ended, 0 otherwise)\n",
    "\n",
    "**Algorithm:**\n",
    "1. Store each experience in a circular buffer of capacity N\n",
    "2. When buffer is full, overwrite oldest experiences (FIFO)\n",
    "3. Sample mini-batches uniformly at random for training\n",
    "4. Use sampled mini-batches for gradient updates\n",
    "\n",
    "**Benefits:**\n",
    "- **Data efficiency**: Each experience used multiple times\n",
    "- **Decorrelation**: Random sampling breaks temporal dependencies\n",
    "- **Stability**: Diverse mini-batches reduce variance in gradient estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experience tuple structure\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Circular buffer for storing and sampling experiences.\n",
    "    \n",
    "    Implements uniform random sampling from stored experiences.\n",
    "    Oldest experiences are automatically overwritten when capacity is reached.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Initialize replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            capacity (int): Maximum number of experiences to store\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add experience to buffer.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state observation\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Resulting state\n",
    "            done: Episode termination flag\n",
    "        \"\"\"\n",
    "        self.buffer.append(Experience(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of experiences.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Number of experiences to sample\n",
    "            \n",
    "        Returns:\n",
    "            tuple: Batch of (states, actions, rewards, next_states, dones)\n",
    "        \"\"\"\n",
    "        # Uniform random sampling without replacement\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Separate components and convert to arrays\n",
    "        states = np.array([e.state for e in experiences])\n",
    "        actions = np.array([e.action for e in experiences])\n",
    "        rewards = np.array([e.reward for e in experiences])\n",
    "        next_states = np.array([e.next_state for e in experiences])\n",
    "        dones = np.array([e.done for e in experiences], dtype=np.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return current buffer size.\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize replay buffer with capacity of 100,000 experiences\n",
    "BUFFER_SIZE = 100_000\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "print(f\"Replay buffer initialized with capacity: {BUFFER_SIZE:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Exploration-Exploitation Strategy\n",
    "\n",
    "### The Exploration-Exploitation Dilemma\n",
    "\n",
    "**Definition:** The exploration-exploitation trade-off is the decision between exploring new actions to discover their rewards (exploration) versus choosing known good actions to maximize immediate reward (exploitation).\n",
    "\n",
    "**Formal Statement:**\n",
    "- **Exploitation**: Select $a^* = \\arg\\max_a Q(s, a)$ (greedy action)\n",
    "- **Exploration**: Select $a \\sim \\text{Uniform}(\\mathcal{A})$ (random action)\n",
    "\n",
    "**Example:** When choosing a restaurant, you can return to your favorite establishment (exploitation) or try a new one that might be better or worse (exploration). Pure exploitation prevents discovering superior alternatives; pure exploration foregoes known rewards.\n",
    "\n",
    "### Epsilon-Greedy Policy\n",
    "\n",
    "**Definition:** An epsilon-greedy policy selects the greedy action with probability (1 - ε) and a random action with probability ε.\n",
    "\n",
    "**Policy Definition:**\n",
    "\n",
    "$$\\pi(a|s) = \\begin{cases} \n",
    "1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|} & \\text{if } a = \\arg\\max_{a'} Q(s, a') \\\\\n",
    "\\frac{\\epsilon}{|\\mathcal{A}|} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "### Epsilon Decay Schedule\n",
    "\n",
    "**Definition:** The exploration rate ε decreases over time according to an exponential decay schedule:\n",
    "\n",
    "$$\\epsilon_t = \\epsilon_{\\text{end}} + (\\epsilon_{\\text{start}} - \\epsilon_{\\text{end}}) \\cdot e^{-t / \\tau}$$\n",
    "\n",
    "Where:\n",
    "- $\\epsilon_{\\text{start}} = 1.0$: Initial exploration rate (100% random)\n",
    "- $\\epsilon_{\\text{end}} = 0.01$: Final exploration rate (1% random)\n",
    "- $\\tau$: Decay time constant\n",
    "- $t$: Current timestep\n",
    "\n",
    "**Rationale:**\n",
    "- **Early training**: High exploration discovers diverse experiences for learning\n",
    "- **Late training**: Low exploration exploits learned policy for performance\n",
    "- **Exponential decay**: Provides rapid initial decrease with gradual stabilization\n",
    "\n",
    "**Example:** A child learning to walk initially tries many different approaches (high exploration), but eventually settles on effective techniques while occasionally experimenting with variations (low exploration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "    \"\"\"\n",
    "    Epsilon-greedy exploration policy with exponential decay.\n",
    "    \n",
    "    Implements time-varying exploration rate for balancing\n",
    "    exploration and exploitation during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, start=1.0, end=0.01, decay=500):\n",
    "        \"\"\"\n",
    "        Initialize epsilon schedule parameters.\n",
    "        \n",
    "        Args:\n",
    "            start (float): Initial epsilon value\n",
    "            end (float): Final epsilon value\n",
    "            decay (int): Decay time constant\n",
    "        \"\"\"\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        self.epsilon = start\n",
    "    \n",
    "    def get_epsilon(self, step):\n",
    "        \"\"\"\n",
    "        Calculate epsilon at given timestep using exponential decay.\n",
    "        \n",
    "        Args:\n",
    "            step (int): Current training timestep\n",
    "            \n",
    "        Returns:\n",
    "            float: Current epsilon value\n",
    "        \"\"\"\n",
    "        self.epsilon = self.end + (self.start - self.end) * np.exp(-step / self.decay)\n",
    "        return self.epsilon\n",
    "    \n",
    "    def select_action(self, state, policy_net, step):\n",
    "        \"\"\"\n",
    "        Select action using epsilon-greedy strategy.\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Current state observation\n",
    "            policy_net (DQN): Q-network for action selection\n",
    "            step (int): Current training timestep\n",
    "            \n",
    "        Returns:\n",
    "            int: Selected action index\n",
    "        \"\"\"\n",
    "        epsilon = self.get_epsilon(step)\n",
    "        \n",
    "        # Exploration: random action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(action_size)\n",
    "        \n",
    "        # Exploitation: greedy action selection\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                q_values = policy_net(state_tensor)\n",
    "                return q_values.argmax(dim=1).item()\n",
    "\n",
    "# Initialize epsilon-greedy policy\n",
    "policy = EpsilonGreedyPolicy(start=1.0, end=0.01, decay=500)\n",
    "\n",
    "print(\"Epsilon-greedy policy initialized\")\n",
    "print(f\"Initial exploration rate: {policy.start}\")\n",
    "print(f\"Final exploration rate: {policy.end}\")\n",
    "print(f\"Decay constant: {policy.decay}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training Algorithm Implementation\n",
    "\n",
    "### DDQN Training Procedure\n",
    "\n",
    "The training algorithm consists of alternating phases of environment interaction and network optimization.\n",
    "\n",
    "**Algorithm Overview:**\n",
    "\n",
    "```\n",
    "For episode = 1 to N:\n",
    "    1. Reset environment, observe initial state s₀\n",
    "    2. For t = 0 to T:\n",
    "        a. Select action aₜ using ε-greedy policy\n",
    "        b. Execute aₜ, observe reward rₜ and next state sₜ₊₁\n",
    "        c. Store transition (sₜ, aₜ, rₜ, sₜ₊₁, dₜ) in replay buffer\n",
    "        d. Sample random mini-batch from replay buffer\n",
    "        e. Compute DDQN targets\n",
    "        f. Perform gradient descent step\n",
    "        g. If episode multiple of C, update target network θ⁻ ← θ\n",
    "```\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "**Definition:** The loss function quantifies the error between predicted Q-values and target Q-values.\n",
    "\n",
    "**Mean Squared Error Loss:**\n",
    "\n",
    "$$L(\\theta) = \\mathbb{E}_{(s,a,r,s',d) \\sim \\mathcal{D}}\\left[(Q(s, a; \\theta) - y)^2\\right]$$\n",
    "\n",
    "Where the target y is computed using DDQN:\n",
    "\n",
    "$$y = \\begin{cases}\n",
    "r & \\text{if episode terminated} \\\\\n",
    "r + \\gamma Q(s', \\arg\\max_{a'} Q(s', a'; \\theta); \\theta^-) & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Gradient Descent Update:**\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L(\\theta)$$\n",
    "\n",
    "Where α is the learning rate.\n",
    "\n",
    "### Hyperparameter Configuration\n",
    "\n",
    "**Learning Rate (α):**\n",
    "- **Definition**: Step size for gradient descent updates\n",
    "- **Value**: 0.0001\n",
    "- **Rationale**: Conservative learning rate ensures stable convergence for deep networks\n",
    "\n",
    "**Discount Factor (γ):**\n",
    "- **Definition**: Weight assigned to future rewards\n",
    "- **Value**: 0.99\n",
    "- **Rationale**: High discount factor appropriate for tasks requiring long-term planning\n",
    "\n",
    "**Batch Size:**\n",
    "- **Definition**: Number of experiences sampled per gradient update\n",
    "- **Value**: 64\n",
    "- **Rationale**: Balances gradient estimate variance with computational efficiency\n",
    "\n",
    "**Target Network Update Frequency:**\n",
    "- **Definition**: Number of episodes between target network updates\n",
    "- **Value**: 10 episodes\n",
    "- **Rationale**: Provides stable learning targets while tracking policy network improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE = 10\n",
    "NUM_EPISODES = 1000\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "# Initialize Adam optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Training hyperparameters:\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Discount factor: {GAMMA}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Target network update frequency: {TARGET_UPDATE} episodes\")\n",
    "print(f\"Total training episodes: {NUM_EPISODES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    \"\"\"\n",
    "    Perform one training step using a mini-batch from replay buffer.\n",
    "    \n",
    "    Implements DDQN target computation and gradient descent update.\n",
    "    \n",
    "    Returns:\n",
    "        float: Loss value for this training step\n",
    "    \"\"\"\n",
    "    # Require minimum buffer size before training\n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # Sample mini-batch from replay buffer\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "    # Compute current Q-values: Q(s, a; θ)\n",
    "    current_q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # Compute DDQN targets\n",
    "    with torch.no_grad():\n",
    "        # Action selection using online network\n",
    "        next_actions = policy_net(next_states).argmax(dim=1)\n",
    "        \n",
    "        # Action evaluation using target network\n",
    "        next_q_values = target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute targets: y = r + γ * Q(s', a*; θ⁻) * (1 - done)\n",
    "        target_q_values = rewards + (GAMMA * next_q_values * (1 - dones))\n",
    "    \n",
    "    # Compute loss: MSE between current Q and target Q\n",
    "    loss = F.mse_loss(current_q_values, target_q_values)\n",
    "    \n",
    "    # Gradient descent step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping for stability\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "print(\"Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Training Execution\n",
    "\n",
    "This section executes the complete training procedure over the specified number of episodes. Training metrics are collected for subsequent analysis.\n",
    "\n",
    "**Expected Training Progression:**\n",
    "- **Phase 1 (Episodes 0-100)**: High exploration, poor performance, rapid learning\n",
    "- **Phase 2 (Episodes 100-300)**: Decreased exploration, improved stability\n",
    "- **Phase 3 (Episodes 300-600)**: Refined policy, approaching solution threshold\n",
    "- **Phase 4 (Episodes 600+)**: Near-optimal policy, consistent high performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracking variables\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "epsilon_history = []\n",
    "\n",
    "step_count = 0\n",
    "best_avg_reward = -float('inf')\n",
    "\n",
    "print(\"Starting training procedure\\n\")\n",
    "\n",
    "# Main training loop\n",
    "for episode in tqdm(range(NUM_EPISODES), desc=\"Training Progress\"):\n",
    "    # Initialize episode\n",
    "    state, _ = env.reset(seed=SEED + episode)\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # Episode execution loop\n",
    "    for step in range(MAX_STEPS):\n",
    "        # Action selection\n",
    "        action = policy.select_action(state, policy_net, step_count)\n",
    "        \n",
    "        # Environment step\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Store experience\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Training step\n",
    "        train_step()\n",
    "        \n",
    "        # Update state and counters\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        step_count += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Record episode statistics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(step + 1)\n",
    "    epsilon_history.append(policy.epsilon)\n",
    "    \n",
    "    # Target network update\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    # Periodic progress reporting\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-50:])\n",
    "        avg_length = np.mean(episode_lengths[-50:])\n",
    "        \n",
    "        print(f\"\\nEpisode {episode + 1}/{NUM_EPISODES}\")\n",
    "        print(f\"Average reward (last 50): {avg_reward:.2f}\")\n",
    "        print(f\"Average length (last 50): {avg_length:.1f}\")\n",
    "        print(f\"Epsilon: {policy.epsilon:.3f}\")\n",
    "        print(f\"Buffer size: {len(replay_buffer):,}\")\n",
    "        \n",
    "        if avg_reward > best_avg_reward:\n",
    "            best_avg_reward = avg_reward\n",
    "            print(f\"New best average reward: {best_avg_reward:.2f}\")\n",
    "\n",
    "print(\"\\nTraining completed\")\n",
    "print(f\"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "print(f\"Best average reward achieved: {best_avg_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Training Analysis and Visualization\n",
    "\n",
    "This section provides quantitative analysis of training performance through multiple visualization perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive training analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Episode rewards with moving average\n",
    "axes[0, 0].plot(episode_rewards, alpha=0.3, label='Episode Reward')\n",
    "moving_avg = np.convolve(episode_rewards, np.ones(50)/50, mode='valid')\n",
    "axes[0, 0].plot(range(49, len(episode_rewards)), moving_avg, \n",
    "                label='Moving Average (50 episodes)', linewidth=2)\n",
    "axes[0, 0].axhline(y=200, color='r', linestyle='--', label='Solved Threshold')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('Episode Rewards Over Training')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Epsilon decay schedule\n",
    "axes[0, 1].plot(epsilon_history, color='orange')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Epsilon')\n",
    "axes[0, 1].set_title('Exploration Rate Decay')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Episode lengths\n",
    "axes[1, 0].plot(episode_lengths, alpha=0.3, label='Episode Length')\n",
    "moving_avg_length = np.convolve(episode_lengths, np.ones(50)/50, mode='valid')\n",
    "axes[1, 0].plot(range(49, len(episode_lengths)), moving_avg_length, \n",
    "                label='Moving Average (50 episodes)', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Steps')\n",
    "axes[1, 0].set_title('Episode Duration Over Training')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Rolling performance metric\n",
    "window_size = 100\n",
    "rolling_avg = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "axes[1, 1].plot(range(window_size-1, len(episode_rewards)), rolling_avg, \n",
    "                linewidth=2, color='green')\n",
    "axes[1, 1].axhline(y=200, color='r', linestyle='--', label='Solved Threshold')\n",
    "axes[1, 1].fill_between(range(window_size-1, len(episode_rewards)), 200, rolling_avg, \n",
    "                        where=(rolling_avg >= 200), alpha=0.3, color='green')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Average Reward (100 episodes)')\n",
    "axes[1, 1].set_title('Rolling Average Performance')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/training_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training metrics visualization generated\")\n",
    "print(\"Saved to: training_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Policy Evaluation and Visualization\n",
    "\n",
    "This section evaluates the trained policy and generates visualizations of the agent's landing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(num_episodes=5, render=True):\n",
    "    \"\"\"\n",
    "    Evaluate trained policy on test episodes.\n",
    "    \n",
    "    Args:\n",
    "        num_episodes (int): Number of test episodes\n",
    "        render (bool): Whether to capture frames\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (test_rewards, frames)\n",
    "    \"\"\"\n",
    "    test_rewards = []\n",
    "    frames = []\n",
    "    \n",
    "    policy_net.eval()\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_frames = []\n",
    "        \n",
    "        for step in range(MAX_STEPS):\n",
    "            # Greedy action selection (no exploration)\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                q_values = policy_net(state_tensor)\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Capture frames for first episode\n",
    "            if render and episode == 0:\n",
    "                frame = env.render()\n",
    "                episode_frames.append(frame)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        test_rewards.append(episode_reward)\n",
    "        print(f\"Test Episode {episode + 1}: Reward = {episode_reward:.2f}, Steps = {step + 1}\")\n",
    "        \n",
    "        if episode == 0:\n",
    "            frames = episode_frames\n",
    "    \n",
    "    print(f\"\\nTest Results Summary:\")\n",
    "    print(f\"Mean reward: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}\")\n",
    "    print(f\"Min reward: {np.min(test_rewards):.2f}\")\n",
    "    print(f\"Max reward: {np.max(test_rewards):.2f}\")\n",
    "    \n",
    "    policy_net.train()\n",
    "    \n",
    "    return test_rewards, frames\n",
    "\n",
    "# Execute policy evaluation\n",
    "print(\"Testing trained policy\\n\")\n",
    "test_rewards, test_frames = test_agent(num_episodes=5, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate landing sequence visualization\n",
    "print(\"\\nGenerating landing sequence visualization\")\n",
    "\n",
    "num_frames_to_show = 6\n",
    "frame_indices = np.linspace(0, len(test_frames)-1, num_frames_to_show, dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, frame_idx in enumerate(frame_indices):\n",
    "    axes[idx].imshow(test_frames[frame_idx])\n",
    "    axes[idx].set_title(f'Step {frame_idx}/{len(test_frames)}')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Lunar Lander: Key Frames from Landing Sequence', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/landing_sequence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Landing sequence visualization saved to: landing_sequence.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Animated GIF Generation\n",
    "\n",
    "This section creates an animated GIF showing the complete landing sequence of the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_landing_gif(frames, output_path, fps=30):\n",
    "    \"\"\"\n",
    "    Create animated GIF from captured frames.\n",
    "    \n",
    "    Args:\n",
    "        frames (list): List of RGB frames\n",
    "        output_path (str): Path for output GIF file\n",
    "        fps (int): Frames per second for animation\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Initialize with first frame\n",
    "    im = ax.imshow(frames[0])\n",
    "    \n",
    "    def update(frame_idx):\n",
    "        \"\"\"Update function for animation.\"\"\"\n",
    "        im.set_array(frames[frame_idx])\n",
    "        ax.set_title(f'Lunar Lander - Step {frame_idx}/{len(frames)}', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        return [im]\n",
    "    \n",
    "    # Create animation\n",
    "    anim = FuncAnimation(fig, update, frames=len(frames), \n",
    "                        interval=1000/fps, blit=True)\n",
    "    \n",
    "    # Save as GIF\n",
    "    writer = PillowWriter(fps=fps)\n",
    "    anim.save(output_path, writer=writer)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"GIF created successfully: {output_path}\")\n",
    "    print(f\"Total frames: {len(frames)}\")\n",
    "    print(f\"Duration: {len(frames)/fps:.1f} seconds\")\n",
    "\n",
    "# Generate GIF\n",
    "print(\"Creating animated GIF of landing sequence\\n\")\n",
    "create_landing_gif(test_frames, '/mnt/user-data/outputs/lunar_landing.gif', fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Model Persistence\n",
    "\n",
    "Save the trained model for future use without requiring retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and training history\n",
    "model_path = '/mnt/user-data/outputs/lunar_lander_ddqn_model.pth'\n",
    "\n",
    "torch.save({\n",
    "    'episode': NUM_EPISODES,\n",
    "    'policy_net_state_dict': policy_net.state_dict(),\n",
    "    'target_net_state_dict': target_net.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'episode_rewards': episode_rewards,\n",
    "    'best_avg_reward': best_avg_reward,\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"\\nTo load the model:\")\n",
    "print(f\"checkpoint = torch.load('{model_path}')\")\n",
    "print(f\"policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "### Implementation Overview\n",
    "\n",
    "This notebook presented a complete implementation of Double Deep Q-Networks for the Lunar Lander environment, including:\n",
    "\n",
    "1. **Environment specification**: 8-dimensional continuous state space, 4 discrete actions\n",
    "2. **DDQN algorithm**: Dual network architecture for reduced overestimation bias\n",
    "3. **Experience replay**: Uniform random sampling from circular buffer\n",
    "4. **Exploration strategy**: Epsilon-greedy policy with exponential decay\n",
    "5. **Neural architecture**: Two-layer feedforward network with ReLU activations\n",
    "\n",
    "### Key Algorithmic Components\n",
    "\n",
    "**Q-Learning Foundation:**\n",
    "- Bellman optimality equation provides theoretical basis\n",
    "- Function approximation enables continuous state spaces\n",
    "- Temporal difference learning for online updates\n",
    "\n",
    "**DDQN Improvements:**\n",
    "- Decoupled action selection and evaluation\n",
    "- Target network for stable learning\n",
    "- Reduced positive bias in value estimates\n",
    "\n",
    "**Training Stabilization:**\n",
    "- Experience replay breaks temporal correlations\n",
    "- Epsilon decay balances exploration and exploitation\n",
    "- Gradient clipping prevents exploding gradients\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "The trained agent achieved:\n",
    "- Average reward over solution threshold (200+)\n",
    "- Consistent successful landings in test episodes\n",
    "- Stable policy after convergence\n",
    "\n",
    "### Extensions and Future Work\n",
    "\n",
    "Potential improvements to the implementation:\n",
    "\n",
    "1. **Prioritized Experience Replay**: Sample important transitions more frequently\n",
    "2. **Dueling DQN**: Separate value and advantage function streams\n",
    "3. **Noisy Networks**: Replace epsilon-greedy with parametric exploration\n",
    "4. **Rainbow DQN**: Integrate multiple algorithmic improvements\n",
    "5. **Distributional RL**: Model full return distribution instead of expected value\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "The techniques demonstrated apply to:\n",
    "- Robotic control systems\n",
    "- Game playing agents\n",
    "- Resource allocation problems\n",
    "- Autonomous navigation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "env.close()\n",
    "print(\"Environment closed. Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
